version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports: ["11434:11434"]
    volumes: ["ollama_models:/root/.ollama"]
    # Request GPU access (optional/best-effort on Docker Desktop/Compose)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 30

  orders-app:
    build:
      context: ./orders-app
    container_name: orders-app
    depends_on:
      ollama:
        condition: service_started
    ports:
      - "8081:8081"
    environment:
      - JAVA_TOOL_OPTIONS=-XX:MaxRAMPercentage=75.0
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8081/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 20

  orders-chat-client:
    build:
      context: ./orders-chat-client
    container_name: orders-chat-client
    depends_on:
      - orders-app
      - ollama
    ports:
      - "8080:8080"
    environment:
      - JAVA_TOOL_OPTIONS=-XX:MaxRAMPercentage=75.0
    # if you want to override model via env:
    #   SPRING_AI_OLLAMA_CHAT_OPTIONS_MODEL=qwen2.5:7b
    entrypoint:
      - /bin/sh
      - -c
      - "while ! (curl -sf http://orders-app:8081/actuator/health >/dev/null 2>&1 || curl -sf http://orders-app:8081/health >/dev/null 2>&1 || nc -z orders-app 8081); do echo 'waiting for orders-app'; sleep 3; done; while ! (curl -sf http://ollama:11434/api/tags >/dev/null 2>&1 || nc -z ollama 11434); do echo 'waiting for ollama'; sleep 3; done; exec java -jar app.jar"

volumes:
  ollama_models: {}
